{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, importlib, torch, cv2, time\n",
    "from demo_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load demo configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device =  cpu\n"
     ]
    }
   ],
   "source": [
    "cfg = CFG()\n",
    "device = cfg.device\n",
    "topk = 5\n",
    "video_file = 'uploads/livedemo.avi'\n",
    "store_vid = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load sign classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {}\n",
    "reader = csv.reader(open(cfg.class_map_path))\n",
    "header = next(reader)\n",
    "for row in reader:\n",
    "    class_map[int(row[0])] = row[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): PartAttentionLayer(\n",
       "      (drop_part): DropPart()\n",
       "      (blocks): ModuleList(\n",
       "        (0): PartAttentionBlock(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MSA(\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (attn_act): ReLU()\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): PartAttentionBlock(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MSA(\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (attn_act): ReLU()\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): PartAttentionBlock(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MSA(\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (attn_act): ReLU()\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): PartAttentionBlock(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MSA(\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (attn_act): ReLU()\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): TemporalMerging()\n",
       "    )\n",
       "    (1): PartAttentionLayer(\n",
       "      (drop_part): DropPart()\n",
       "      (blocks): ModuleList(\n",
       "        (0): PartAttentionBlock(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MSA(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (attn_act): ReLU()\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): PartAttentionBlock(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MSA(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (attn_act): ReLU()\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): PartAttentionBlock(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MSA(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (attn_act): ReLU()\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): PartAttentionBlock(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MSA(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (attn_act): ReLU()\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): TemporalMerging()\n",
       "    )\n",
       "    (2): PartAttentionLayer(\n",
       "      (drop_part): DropPart()\n",
       "      (blocks): ModuleList(\n",
       "        (0): PartAttentionBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MSA(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (attn_act): ReLU()\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): PartAttentionBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MSA(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (attn_act): ReLU()\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): PartAttentionBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MSA(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (attn_act): ReLU()\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): PartAttentionBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MSA(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (attn_act): ReLU()\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): PartAttentionBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MSA(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (attn_act): ReLU()\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): PartAttentionBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MSA(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (attn_act): ReLU()\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): PartAttentionBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MSA(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (attn_act): ReLU()\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): PartAttentionBlock(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MSA(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (attn_act): ReLU()\n",
       "          )\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): FeedForward(\n",
       "            (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AvgPool1d(kernel_size=(3072,), stride=(3072,), padding=(0,))\n",
       "  (head): Linear(in_features=512, out_features=2002, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = importlib.import_module('models.'+cfg.model_type)\n",
    "model = getattr(module, 'Model')(*cfg.model_params.get_model_params())\n",
    "model.load_state_dict(torch.load(cfg.save_model_path, map_location=device)['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recognise video function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognise(video):\n",
    "    vid_feat = torch.tensor(cfg.test_transform(get_video_data(video)), dtype=torch.float32).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output_list = torch.argsort(model(vid_feat.to(device)).squeeze(0)).cpu().tolist()\n",
    "    results = {}\n",
    "    for i in range(1, topk+1):\n",
    "        results[f'word{i}'] = class_map[output_list[-i]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### capture video and recognise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 results - \n",
      "word1 : A\n",
      "word2 : Funny\n",
      "word3 : Thursday\n",
      "word4 : Ghee\n",
      "word5 : Swimming\n"
     ]
    }
   ],
   "source": [
    "record_time = 5\n",
    "wait_time = 3\n",
    "record_start = False\n",
    "video = []\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "width, height = 1280, (1280//4)*3\n",
    "cv2.namedWindow(\"Video\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Video\", width, height)\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "if store_vid:\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    out = cv2.VideoWriter(video_file, cv2.VideoWriter_fourcc(\n",
    "        'M', 'J', 'P', 'G'), fps, (frame_width, frame_height))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "    is_read, image = cap.read()\n",
    "\n",
    "    if not is_read:\n",
    "        break\n",
    "\n",
    "    video.append(image)\n",
    "\n",
    "    if store_vid and record_start:\n",
    "        out.write(image)\n",
    "        out.write(image)\n",
    "        tmp_time = time.time() - rec_start_time\n",
    "        cv2.putText(image, str(int(record_time + 1 - tmp_time)), org=(50, 80), fontFace=font, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "        if tmp_time > record_time:\n",
    "            break\n",
    "\n",
    "    if not record_start:\n",
    "        height, width, channels = image.shape\n",
    "        white_image = np.ones((height, width, channels), dtype=np.uint8) * 255\n",
    "        alpha = 0.5\n",
    "        image = cv2.addWeighted(image, 1 - alpha, white_image, alpha, 0)\n",
    "\n",
    "        tmp_time = time.time() - start_time\n",
    "        cv2.putText(image, str(int(wait_time + 1 - tmp_time)), org=(50, 100), fontFace=font, fontScale=3, color=(0, 0, 255), thickness=10)\n",
    "        if tmp_time > wait_time:\n",
    "            record_start = True\n",
    "            rec_start_time = time.time()\n",
    "\n",
    "    cv2.imshow('Video', image)\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "if store_vid:\n",
    "    out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print('Top 5 results - ')\n",
    "results = recognise(video_file)\n",
    "for i in results.keys():\n",
    "    print(i+' : '+results[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
